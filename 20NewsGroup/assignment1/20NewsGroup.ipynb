{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 <br>\n",
    "> download the \"20 News Group\" dataset from the http://qwone.com/~jason/20Newsgroups/ website.\n",
    ">Note: please download the following dataset to maintain the consistency.\n",
    "> 20news-bydate.tar.gz-20NewsGroups sorted by date; duplicats and som headers removed(18846 documents)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rasika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rasika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/rasika/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rasika/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/rasika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some import imports\n",
    "\n",
    "import pandas as pd                                                    # for create df and write to csv files\n",
    "import shutil                                                          #copy files from one directory to anothers\n",
    "import glob                                                            # access to directories\n",
    "import os, os.path,re \n",
    "import random                                                          # for suffering the list\n",
    "import nltk \n",
    "from sklearn.feature_extraction.text import CountVectorizer            #for create feature matrix\n",
    "from nltk.corpus import stopwords                                      # english stop words\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2<br>\n",
    "> Create five main classes by combining the document in the subclasses in the both training and testing folders.\n",
    "> For examples combine the document in the comp.graphics, comp.os.ms-windows.mics,comp.sys.ibm.pc.hardware, comp.sys.mac.hardware and comp.windows.X classes create one document set for the<b> computer</b> class.Similary, create <b>recreational, science and talk classes </b>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['computer','recreational','science', 'talk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './classes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer files:  4891\n",
      "recreational files:  3116\n",
      "science files:  3951\n",
      "talk files:  3253\n"
     ]
    }
   ],
   "source": [
    "for class_file in classes:\n",
    "    p = os.path.join(root,class_file)\n",
    "    for file_name in glob.iglob('20news-bydate*/'+class_file[:3]+'*/*', recursive=True):\n",
    "        shutil.copy(file_name, p)\n",
    "    \n",
    "    print(class_file +\" files: \" ,len([name for name in os.listdir(p) if os.path.isfile(os.path.join(p, name))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3<br>\n",
    ">split douments randomly in each class into 2 separate dataset, i.e train set and test set.\n",
    "Note: divied the document set in each class with 70(train) & 30(test) ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datafiles(files):\n",
    "    random.Random(101).shuffle(files)\n",
    "    train_split = int(0.7 * len(files))\n",
    "    test_split = int(0.3 * len(files))\n",
    "    train_files=files[:train_split]\n",
    "    test_files=files[-test_split:]\n",
    "    return train_files,test_files # return the list of files names train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 4<br>\n",
    ">Pre-process each document and transform it to a feature vector. Finally, save all the document into multidimensional array in python. Then ,save each class into 2 separate CSV files. For examples, computer_train.csv and computer_test.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=[\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \n",
    "       \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \n",
    "       \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \n",
    "       \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\n",
    "       \"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\",\n",
    "       \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \n",
    "       \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \n",
    "       \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "       \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \n",
    "       \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \n",
    "       \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \n",
    "       \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "       \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\",\n",
    "       \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \n",
    "       \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \n",
    "       \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \n",
    "       \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "       \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
    "       \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \n",
    "       \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \n",
    "       \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \n",
    "       \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \n",
    "       \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \n",
    "       \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \n",
    "       \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \n",
    "       \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\",\n",
    "       \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \n",
    "       \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \n",
    "       \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \n",
    "       \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \n",
    "       \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \n",
    "       \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "       \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\",\n",
    "       \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \n",
    "       \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \n",
    "       \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\",\n",
    "       \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\",\n",
    "       \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\",\n",
    "       \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \n",
    "       \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\",\n",
    "       \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\",\n",
    "       \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + list(string.punctuation) + stop) # english stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = words.words()\n",
    "stemmer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileContent(path,file_name):\n",
    "    with open(path+\"/\"+file_name, 'r', encoding = \"ISO-8859-1\") as theFile:\n",
    "        data = theFile.read().replace('\\n', '')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileWordsList(p,files_name):\n",
    "    words=[]\n",
    "    \n",
    "    for file_name in files_name:\n",
    "        text = getFileContent(p,file_name)\n",
    "        words+= [stemmer.lemmatize(i) for i in word_tokenize(text.lower()) if i not in stop_words and i in word_list\n",
    "                and len(i) > 1] \n",
    "    \n",
    "    return list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWordList():\n",
    "    uWords_train = []\n",
    "    uWords_test = []\n",
    "    \n",
    "    for class_file in classes:\n",
    "        p = os.path.join(root,class_file)\n",
    "        files= [name for name in os.listdir(p) if os.path.isfile(os.path.join(p, name))]\n",
    "        train_files,test_files = split_datafiles(files)\n",
    "        print(\"progress \"+ class_file)\n",
    "        uWords_train += getFileWordsList(p,train_files)\n",
    "        print(' number of unique words '+ class_file +' train is :', len(set(uWords_train)))\n",
    "        uWords_test += getFileWordsList(p,test_files)\n",
    "        print(' number of unique words '+ class_file +' test is : ', len(set(uWords_test)))\n",
    "      \n",
    "    return list(set(uWords_train)), list(set(uWords_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress computer\n",
      " number of unique words computer train is : 8569\n",
      " number of unique words computer test is :  9521\n",
      "progress recreational\n",
      " number of unique words recreational train is : 12608\n",
      " number of unique words recreational test is :  13288\n",
      "progress science\n",
      " number of unique words science train is : 17318\n",
      " number of unique words science test is :  18084\n",
      "progress talk\n",
      " number of unique words talk train is : 20894\n",
      " number of unique words talk test is :  21476\n"
     ]
    }
   ],
   "source": [
    "uniq_words_train, uniq_words_test = getUniqueWordList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train = CountVectorizer(vocabulary=uniq_words_train)\n",
    "cv_test = CountVectorizer(vocabulary=uniq_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vector(root,class_file):\n",
    "    \n",
    "    p = os.path.join(root,class_file)\n",
    "    files= [name for name in os.listdir(p) if os.path.isfile(os.path.join(p, name))]\n",
    "    train_files,test_files = split_datafiles(files)\n",
    "    text_files=[]\n",
    "    for file in train_files:\n",
    "        text_files.append(getFileContent(p,file))\n",
    "        \n",
    "    X = cv_train.fit_transform(text_files)\n",
    "    Y = cv_train.get_feature_names()\n",
    "    df = pd.DataFrame(data=X.toarray(), columns=Y)\n",
    "    df.to_csv('./csv/'+class_file+'_train.csv')\n",
    "    print('created '+class_file+' train csv')\n",
    "    \n",
    "    text_files=[]\n",
    "    for file in test_files:\n",
    "        text_files.append(getFileContent(p,file))\n",
    "            \n",
    "    X = cv_test.fit_transform(text_files)\n",
    "    Y = cv_test.get_feature_names()\n",
    "    df = pd.DataFrame(data=X.toarray(), columns=Y)\n",
    "    df.to_csv('./csv/'+class_file+'_test.csv')\n",
    "    print('created '+class_file+' test csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created computer train csv\n",
      "created computer test csv\n",
      "created recreational train csv\n",
      "created recreational test csv\n",
      "created science train csv\n",
      "created science test csv\n",
      "created talk train csv\n",
      "created talk test csv\n"
     ]
    }
   ],
   "source": [
    "create_feature_vector(root,classes[0])\n",
    "create_feature_vector(root,classes[1])\n",
    "create_feature_vector(root,classes[2])\n",
    "create_feature_vector(root,classes[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
